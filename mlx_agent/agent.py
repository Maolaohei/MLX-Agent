"""
MLX-Agent ä¸»ç±» - v0.3.0 ç”Ÿäº§å°±ç»ªç‰ˆ

æ ¸å¿ƒåŠŸèƒ½ï¼š
- ä¼˜é›…å…³é—­æœºåˆ¶
- æµå¼è¾“å‡ºæ”¯æŒ
- ChromaDB è®°å¿†ç³»ç»Ÿ
- å¥åº·æ£€æŸ¥ç«¯ç‚¹
- å®Œå–„çš„é”™è¯¯å¤„ç†
"""

import asyncio
import signal
import time
from pathlib import Path
from typing import Optional, Dict, Any, AsyncGenerator
from contextlib import asynccontextmanager

from loguru import logger

from .config import Config
from .memory import ChromaMemorySystem
from .memory.consolidation import MemoryConsolidator
from .identity import IdentityManager
from .compression import TokenCompressor
from .skills import SkillRegistry
from .skills.compat.openclaw import OpenClawSkillAdapter
from .tasks import TaskQueue, TaskWorker, TaskExecutor, TaskPriority, Task, TaskResult
from .chat import ChatSessionManager, ChatResponse
from .llm import LLMClient
from .api_manager import APIManager, get_api_manager
from .health import HealthCheckServer


class MLXAgent:
    """MLX-Agent ä¸»ç±» - ç”Ÿäº§å°±ç»ªç‰ˆæœ¬
    
    é«˜æ€§èƒ½ã€è½»é‡çº§ã€å¤šå¹³å° AI Agent
    å¸¦æœ‰äººè®¾å®ˆæŠ¤ã€Token å‹ç¼©ã€è®°å¿†æ•´åˆç­‰é«˜çº§ç‰¹æ€§
    
    Example:
        >>> agent = MLXAgent(config_path="config.yaml")
        >>> await agent.start()
    """
    
    def __init__(self, config_path: Optional[str] = None):
        """åˆå§‹åŒ– MLX-Agent
        
        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„ï¼Œé»˜è®¤ä½¿ç”¨ config/config.yaml
        """
        # ä½¿ç”¨ uvloop åŠ é€Ÿ asyncio (å¦‚æœæ”¯æŒ)
        try:
            import uvloop
            asyncio.set_event_loop_policy(uvloop.EventLoopPolicy())
            logger.debug("Using uvloop")
        except ImportError:
            pass
        
        # åˆ›å»ºæ–°çš„äº‹ä»¶å¾ªç¯
        try:
            self.loop = asyncio.get_event_loop()
            if self.loop.is_closed():
                self.loop = asyncio.new_event_loop()
                asyncio.set_event_loop(self.loop)
        except RuntimeError:
            self.loop = asyncio.new_event_loop()
            asyncio.set_event_loop(self.loop)
        
        # åŠ è½½é…ç½®
        try:
            self.config = Config.load(config_path)
        except Exception as e:
            logger.warning(f"Failed to load config: {e}, using defaults")
            self.config = Config()
        
        # å…³é—­äº‹ä»¶
        self._shutdown_event = asyncio.Event()
        self._shutdown_timeout = 30  # ä¼˜é›…å…³é—­è¶…æ—¶ï¼ˆç§’ï¼‰
        self._running = False
        
        # åˆå§‹åŒ–ç»„ä»¶ï¼ˆå°†åœ¨ start ä¸­åˆå§‹åŒ–ï¼‰
        self.memory: Optional[ChromaMemorySystem] = None
        self.consolidator: Optional[MemoryConsolidator] = None
        self.identity: Optional[IdentityManager] = None
        self.compressor: Optional[TokenCompressor] = None
        self.skills: Optional[SkillRegistry] = None
        self.openclaw_skills: Optional[OpenClawSkillAdapter] = None
        self.telegram: Optional[Any] = None
        self.api_manager: Optional[APIManager] = None
        self.health_server: Optional[HealthCheckServer] = None
        
        # ä»»åŠ¡ç³»ç»Ÿ
        self.task_queue: Optional[TaskQueue] = None
        self.task_executor: Optional[TaskExecutor] = None
        self.task_worker: Optional[TaskWorker] = None
        self.chat_manager: Optional[ChatSessionManager] = None
        
        # LLM å®¢æˆ·ç«¯
        self.llm: Optional[LLMClient] = None
        
        # è®¾ç½®ä¿¡å·å¤„ç†
        self._setup_signal_handlers()
        
        logger.info(f"MLX-Agent v{self.config.version} initialized")
    
    def _setup_signal_handlers(self):
        """è®¾ç½®ä¿¡å·å¤„ç†å™¨ - ä¼˜é›…å…³é—­"""
        def signal_handler(sig):
            logger.info(f"Received signal {sig.name}, initiating graceful shutdown...")
            # ä½¿ç”¨ call_soon_threadsafe ç¡®ä¿çº¿ç¨‹å®‰å…¨
            self.loop.call_soon_threadsafe(self._shutdown_event.set)
        
        for sig in (signal.SIGTERM, signal.SIGINT):
            try:
                self.loop.add_signal_handler(sig, lambda s=sig: signal_handler(s))
            except Exception as e:
                logger.warning(f"Failed to set signal handler for {sig}: {e}")
    
    async def start(self):
        """å¯åŠ¨ Agent - å¸¦é”™è¯¯æ¢å¤"""
        if self._running:
            logger.warning("Agent already running")
            return
        
        logger.info("Starting MLX-Agent...")
        self._running = True
        self._shutdown_event.clear()
        
        start_time = time.time()
        
        try:
            # 1. åˆå§‹åŒ– API ç®¡ç†å™¨ï¼ˆæœ€å…ˆåŠ è½½ï¼Œå…¶ä»–ç»„ä»¶ä¾èµ–ï¼‰
            await self._init_api_manager()
            
            # 2. åˆå§‹åŒ–äººè®¾ç®¡ç†å™¨
            await self._init_identity()
            
            # 3. åˆå§‹åŒ– Token å‹ç¼©å™¨
            await self._init_compressor()
            
            # 4. åˆå§‹åŒ–è®°å¿†ç³»ç»Ÿ
            await self._init_memory()
            
            # 5. åˆå§‹åŒ–è®°å¿†æ•´åˆå™¨
            await self._init_consolidator()
            
            # 6. åˆå§‹åŒ– LLM å®¢æˆ·ç«¯
            await self._init_llm()
            
            # 7. åˆå§‹åŒ–æŠ€èƒ½ç³»ç»Ÿ
            await self._init_skills()
            
            # 8. åˆå§‹åŒ–ä»»åŠ¡ç³»ç»Ÿ
            await self._init_task_system()
            
            # 9. åˆå§‹åŒ–å¹³å°é€‚é…å™¨
            await self._init_platforms()
            
            # 10. å¯åŠ¨å¥åº·æ£€æŸ¥æœåŠ¡å™¨
            await self._init_health_server()
            
            elapsed = time.time() - start_time
            logger.info(f"MLX-Agent started successfully in {elapsed:.2f}s!")
            
            # å¯åŠ¨å®šæ—¶ä»»åŠ¡
            asyncio.create_task(self._scheduled_tasks())
            
            # ä¿æŒè¿è¡Œï¼Œç›´åˆ°æ”¶åˆ°å…³é—­ä¿¡å·
            await self._run_main_loop()
                
        except Exception as e:
            logger.error(f"Failed to start agent: {e}")
            await self.stop()
            raise
    
    async def stop(self):
        """ä¼˜é›…åœæ­¢ Agent - æœ‰åºå…³é—­æ‰€æœ‰èµ„æº"""
        if not self._running:
            return
        
        logger.info("Initiating graceful shutdown...")
        self._running = False
        self._shutdown_event.set()
        
        shutdown_tasks = []
        
        # æŒ‰ä¾èµ–é¡ºåºå…³é—­ç»„ä»¶
        # 1. åœæ­¢æ¥å—æ–°è¿æ¥ (å¥åº·æ£€æŸ¥)
        if self.health_server:
            logger.info("Stopping health check server...")
            shutdown_tasks.append(asyncio.create_task(
                self._safe_stop("health_server", self.health_server.stop())
            ))
        
        # 2. åœæ­¢å¹³å°é€‚é…å™¨
        if self.telegram:
            logger.info("Stopping Telegram adapter...")
            shutdown_tasks.append(asyncio.create_task(
                self._safe_stop("telegram", self.telegram.stop())
            ))
        
        # 3. åœæ­¢ä»»åŠ¡ç³»ç»Ÿï¼ˆç­‰å¾…ä»»åŠ¡å®Œæˆæˆ–è¶…æ—¶ï¼‰
        if self.task_worker:
            logger.info("Stopping task worker...")
            shutdown_tasks.append(asyncio.create_task(
                self._safe_stop("task_worker", self.task_worker.stop())
            ))
        
        if self.task_queue:
            logger.info("Shutting down task queue...")
            shutdown_tasks.append(asyncio.create_task(
                self._safe_stop("task_queue", self.task_queue.shutdown())
            ))
        
        if self.task_executor:
            logger.info("Shutting down task executor...")
            shutdown_tasks.append(asyncio.create_task(
                self._safe_stop("task_executor", self.task_executor.shutdown())
            ))
        
        # 4. å…³é—­æŠ€èƒ½ç³»ç»Ÿ
        if self.skills:
            logger.info("Closing skills...")
            shutdown_tasks.append(asyncio.create_task(
                self._safe_stop("skills", self.skills.close())
            ))
        
        # 5. å…³é—­ LLM å®¢æˆ·ç«¯
        if self.llm:
            logger.info("Closing LLM client...")
            shutdown_tasks.append(asyncio.create_task(
                self._safe_stop("llm", self.llm.close())
            ))
        
        # 6. å…³é—­è®°å¿†ç³»ç»Ÿ
        if self.memory:
            logger.info("Closing memory system...")
            shutdown_tasks.append(asyncio.create_task(
                self._safe_stop("memory", self.memory.close())
            ))
        
        # 7. å…³é—­ API ç®¡ç†å™¨
        if self.api_manager:
            logger.info("Closing API manager...")
            shutdown_tasks.append(asyncio.create_task(
                self._safe_stop("api_manager", self.api_manager.close())
            ))
        
        # ç­‰å¾…æ‰€æœ‰å…³é—­ä»»åŠ¡å®Œæˆï¼ˆå¸¦è¶…æ—¶ï¼‰
        if shutdown_tasks:
            try:
                await asyncio.wait_for(
                    asyncio.gather(*shutdown_tasks, return_exceptions=True),
                    timeout=self._shutdown_timeout
                )
            except asyncio.TimeoutError:
                logger.warning(f"Shutdown timeout ({self._shutdown_timeout}s) exceeded")
        
        logger.info("MLX-Agent stopped")
    
    async def _safe_stop(self, name: str, coro):
        """å®‰å…¨åœæ­¢ç»„ä»¶ï¼ˆæ•è·å¼‚å¸¸ï¼‰"""
        try:
            await coro
            logger.debug(f"{name} stopped successfully")
        except Exception as e:
            logger.warning(f"Error stopping {name}: {e}")
    
    async def _run_main_loop(self):
        """ä¸»è¿è¡Œå¾ªç¯"""
        try:
            await self._shutdown_event.wait()
        except asyncio.CancelledError:
            pass
        finally:
            await self.stop()
    
    # ============== ç»„ä»¶åˆå§‹åŒ– ==============
    
    async def _init_api_manager(self):
        """åˆå§‹åŒ– API ç®¡ç†å™¨"""
        self.api_manager = get_api_manager()
        await self.api_manager.initialize()
        logger.info("API manager initialized")
    
    async def _init_identity(self):
        """åˆå§‹åŒ–äººè®¾ç®¡ç†å™¨"""
        self.identity = IdentityManager(Path(self.config.memory.path).parent)
        await self.identity.load()
        logger.info(f"Identity loaded: {self.identity.get_identity_summary()}")
    
    async def _init_compressor(self):
        """åˆå§‹åŒ– Token å‹ç¼©å™¨"""
        self.compressor = TokenCompressor(model=self.config.llm.model or "gpt-4")
        logger.info("Token compressor initialized")
    
    async def _init_memory(self):
        """åˆå§‹åŒ–è®°å¿†ç³»ç»Ÿ"""
        # ä»é…ç½®è·å–åµŒå…¥æä¾›å•†
        embedding_provider = getattr(self.config.memory, 'embedding_provider', 'local')
        
        self.memory = ChromaMemorySystem(
            path=self.config.memory.path,
            embedding_provider=embedding_provider,
            auto_archive=True
        )
        await self.memory.initialize()
        logger.info("ChromaDB memory system initialized")
    
    async def _init_consolidator(self):
        """åˆå§‹åŒ–è®°å¿†æ•´åˆå™¨"""
        self.consolidator = MemoryConsolidator(
            Path(self.config.memory.path),
            similarity_threshold=0.7
        )
        logger.info("Memory consolidator initialized")
    
    async def _init_llm(self):
        """åˆå§‹åŒ– LLM å®¢æˆ·ç«¯"""
        try:
            primary_config = None
            fallback_config = None
            failover_enabled = False
            
            if hasattr(self.config.llm, 'primary') and self.config.llm.primary:
                logger.info("Using multi-model configuration")
                primary_data = self.config.llm.primary
                primary_config = {
                    'api_key': primary_data.api_key,
                    'api_base': primary_data.api_base,
                    'auth_token': primary_data.auth_token,
                    'model': primary_data.model,
                    'temperature': primary_data.temperature,
                    'max_tokens': primary_data.max_tokens,
                }
                
                if self.config.llm.fallback:
                    fallback_data = self.config.llm.fallback
                    fallback_config = {
                        'api_key': fallback_data.api_key,
                        'api_base': fallback_data.api_base,
                        'auth_token': fallback_data.auth_token,
                        'model': fallback_data.model,
                        'temperature': fallback_data.temperature,
                        'max_tokens': fallback_data.max_tokens,
                    }
                
                failover_enabled = self.config.llm.failover.enabled
            else:
                # å…¼å®¹æ—§é…ç½®
                logger.info("Using legacy LLM configuration")
                primary_config = {
                    'api_key': self.config.llm.api_key,
                    'api_base': self.config.llm.api_base,
                    'auth_token': self.config.llm.auth_token,
                    'model': self.config.llm.model,
                    'temperature': self.config.llm.temperature,
                    'max_tokens': self.config.llm.max_tokens,
                }
            
            if primary_config and primary_config.get('api_key'):
                self.llm = LLMClient(
                    primary_config=primary_config,
                    fallback_config=fallback_config,
                    failover_enabled=failover_enabled,
                    max_retries=3
                )
                logger.info(f"LLM client initialized: {primary_config.get('model')}")
            else:
                logger.error("LLM config missing API Key")
                
        except Exception as e:
            logger.error(f"Failed to initialize LLM: {e}")
            raise
    
    async def _init_skills(self):
        """åˆå§‹åŒ–æŠ€èƒ½ç³»ç»Ÿ"""
        self.skills = SkillRegistry(self)
        await self.skills.initialize()
        logger.info("Skill system initialized")
        
        # OpenClaw å…¼å®¹å±‚
        self.openclaw_skills = OpenClawSkillAdapter()
        await self.openclaw_skills.initialize()
        oc_skills = self.openclaw_skills.list_skills()
        logger.info(f"OpenClaw adapter initialized with {len(oc_skills)} skills")
    
    async def _init_task_system(self):
        """åˆå§‹åŒ–ä»»åŠ¡ç³»ç»Ÿ"""
        self.task_queue = TaskQueue(maxsize=1000)
        self.task_executor = TaskExecutor(max_workers=4)
        self.task_worker = TaskWorker(
            queue=self.task_queue,
            executor=self.task_executor,
            num_workers=2,
            default_callback=self._on_task_complete
        )
        await self.task_worker.start()
        
        self.chat_manager = ChatSessionManager(
            task_queue=self.task_queue,
            quick_handler=self._quick_handle_message,
            slow_handler=self._slow_handle_message
        )
        logger.info("Task system initialized")
    
    async def _init_platforms(self):
        """åˆå§‹åŒ–å¹³å°é€‚é…å™¨"""
        if self.config.platforms.telegram.enabled:
            from .platforms.telegram import TelegramAdapter
            self.telegram = TelegramAdapter(
                self.config.platforms.telegram,
                self
            )
            await self.telegram.initialize()
            asyncio.create_task(self.telegram.start())
            logger.info("Telegram adapter started")
    
    async def _init_health_server(self):
        """åˆå§‹åŒ–å¥åº·æ£€æŸ¥æœåŠ¡å™¨"""
        # ä»é…ç½®è·å–å¥åº·æ£€æŸ¥ç«¯å£
        health_port = getattr(self.config, 'health_check', {}).get('port', 8080)
        
        self.health_server = HealthCheckServer(self, port=health_port)
        await self.health_server.start()
    
    # ============== æ¶ˆæ¯å¤„ç† ==============
    
    async def _quick_handle_message(self, text: str, context: dict = None, history: list = None, **kwargs) -> Optional[str]:
        """å¿«é€Ÿæ¶ˆæ¯å¤„ç†å™¨"""
        text_lower = text.lower().strip()
        
        # å¸®åŠ©å‘½ä»¤
        if text_lower in ['/help', 'help', 'å¸®åŠ©']:
            return (
                "ğŸ¤– MLX-Agent å¸®åŠ©\n\n"
                "ğŸ’¬ å¿«é€Ÿå“åº”:\n"
                "â€¢ /help - æ˜¾ç¤ºå¸®åŠ©\n"
                "â€¢ /status - æŸ¥çœ‹çŠ¶æ€\n"
                "â€¢ /tasks - æŸ¥çœ‹è¿›è¡Œä¸­çš„ä»»åŠ¡\n\n"
                "â³ æ…¢é€Ÿä»»åŠ¡ä¼šè‡ªåŠ¨è¿›å…¥é˜Ÿåˆ—ï¼Œå®Œæˆåé€šçŸ¥ä½ ~"
            )
        
        # çŠ¶æ€å‘½ä»¤
        if text_lower in ['/status', 'status', 'çŠ¶æ€']:
            stats = await self.get_stats()
            queue_stats = self.task_queue.get_stats() if self.task_queue else {}
            return (
                f"ğŸ“Š çŠ¶æ€\n"
                f"â€¢ Agent: {'è¿è¡Œä¸­' if stats['running'] else 'å·²åœæ­¢'}\n"
                f"â€¢ ä»»åŠ¡é˜Ÿåˆ—: {queue_stats.get('pending', 0)} ç­‰å¾… / "
                f"{queue_stats.get('running', 0)} æ‰§è¡Œä¸­\n"
                f"â€¢ Skills: {stats['skills']['native']} åŸç”Ÿ / "
                f"{stats['skills']['openclaw']} OpenClaw"
            )
        
        # ä»»åŠ¡åˆ—è¡¨å‘½ä»¤
        if text_lower in ['/tasks', 'tasks', 'ä»»åŠ¡']:
            if context and self.task_queue:
                user_id = context.get('user_id')
                tasks = self.task_queue.get_user_tasks(user_id)
                if tasks:
                    task_list = "\n".join([
                        f"â€¢ {t.id}: {t.status.value} ({t.type})"
                        for t in tasks[:10]
                    ])
                    return f"ğŸ“‹ ä½ çš„ä»»åŠ¡ ({len(tasks)}):\n{task_list}"
                return "ğŸ“‹ å½“å‰æ²¡æœ‰è¿›è¡Œä¸­çš„ä»»åŠ¡"
        
        # ç®€å•çš„é—®å€™è¯­
        greetings = ['hello', 'hi', 'ä½ å¥½', 'æ‚¨å¥½', 'åœ¨å—', 'åœ¨ï¼Ÿ']
        if any(g in text_lower for g in greetings):
            return "ğŸ‘‹ ä½ å¥½ï¼æˆ‘æ˜¯ MLX-Agentï¼Œæœ‰ä»€ä¹ˆå¯ä»¥å¸®ä½ çš„å—ï¼Ÿ"
        
        # çŸ­æ¶ˆæ¯ä½¿ç”¨ LLM å›å¤
        if len(text) < 50 and self.llm:
            try:
                base_prompt = "ç®€çŸ­å›å¤ã€‚"
                if self.identity:
                    system_prompt = self.identity.inject_to_prompt(base_prompt)
                else:
                    system_prompt = base_prompt
                
                response = await self.llm.simple_chat(text, system_prompt)
                return response
            except Exception as e:
                logger.error(f"Quick LLM call failed: {e}")
                return None
        
        return None
    
    async def _slow_handle_message(
        self,
        text: str,
        context: dict = None,
        task: Task = None,
        history: list = None,
        **kwargs
    ) -> str:
        """æ…¢é€Ÿæ¶ˆæ¯å¤„ç†å™¨ - ä½¿ç”¨ LLM ç”Ÿæˆæ™ºèƒ½å›å¤"""
        
        if task:
            task.set_progress("ğŸ¤” æ­£åœ¨ç†è§£ä½ çš„é—®é¢˜...", 0.1)
        
        messages = []
        history = history or []
        
        # æœç´¢ç›¸å…³è®°å¿†
        memories = []
        if self.memory:
            try:
                memories = await self.memory.search(text, top_k=3)
                if task:
                    task.set_progress("ğŸ” æœç´¢ç›¸å…³è®°å¿†...", 0.3)
            except Exception as e:
                logger.warning(f"Memory search failed: {e}")
        
        # æ„å»ºç³»ç»Ÿæç¤º
        base_prompt = "ä½ æ˜¯ MLX-Agentï¼Œä¸€ä¸ªå¼ºå¤§çš„ AI åŠ©æ‰‹ã€‚è¯·ä¿æŒå¯¹è¯è¿è´¯æ€§ï¼Œå‚è€ƒä¹‹å‰çš„å¯¹è¯å†å²ã€‚"
        
        if self.identity:
            system_prompt = self.identity.inject_to_prompt(base_prompt)
        else:
            system_prompt = base_prompt
        
        current_model = "unknown"
        if self.llm:
            current_model = self.llm.get_current_model()
            system_prompt += f"\n\nå½“å‰ä½¿ç”¨çš„æ¨¡å‹: {current_model}"
        
        if memories:
            memory_context = "\n\nç›¸å…³è®°å¿†:\n" + "\n".join([f"- {m.get('content', '')[:100]}" for m in memories[:3]])
            system_prompt += memory_context
        
        messages.append({"role": "system", "content": system_prompt})
        
        # æ·»åŠ å†å²å¯¹è¯
        if history:
            history_to_use = [m for m in history if m.get("role") in ["user", "assistant", "tool"]][-20:]
            messages.extend(history_to_use)
        
        messages.append({"role": "user", "content": text})
        
        # è·å–å¯ç”¨å·¥å…·
        tools = None
        if self.skills:
            try:
                tools = self.skills.get_tools_schema()
            except Exception as e:
                logger.error(f"Failed to get tools: {e}")
        
        if task:
            task.set_progress("ğŸ§  è°ƒç”¨ AI ç”Ÿæˆå›å¤...", 0.6)
        
        # LLM äº¤äº’å¾ªç¯
        max_turns = 5
        turn_count = 0
        
        while turn_count < max_turns:
            turn_count += 1
            
            if not self.llm:
                return f"æ”¶åˆ°ä½ çš„æ¶ˆæ¯: {text[:100]}...\n\nï¼ˆLLM æœªé…ç½®ï¼Œæ— æ³•ç”Ÿæˆæ™ºèƒ½å›å¤ï¼‰"
            
            try:
                use_reasoning = tools is not None and len(tools) > 0
                
                response_msg = await self.llm.chat(
                    messages=messages,
                    tools=tools,
                    tool_choice="auto" if tools else None,
                    reasoning=use_reasoning
                )
                
                tool_calls = response_msg.get("tool_calls")
                content = response_msg.get("content")
                
                assistant_msg = {
                    "role": "assistant",
                    "content": content
                }
                if tool_calls:
                    assistant_msg["tool_calls"] = tool_calls
                
                messages.append(assistant_msg)
                
                if not tool_calls:
                    if task:
                        task.set_progress("âœ¨ å®Œæˆ", 1.0)
                    return content or "ï¼ˆAI æœªè¿”å›ä»»ä½•å†…å®¹ï¼‰"
                
                # æ‰§è¡Œå·¥å…·è°ƒç”¨
                if task:
                    task.set_progress(f"ğŸ”§ æ‰§è¡Œå·¥å…· ({len(tool_calls)} ä¸ª)...", 0.8)
                
                for tool_call in tool_calls:
                    function_name = tool_call.get("function", {}).get("name")
                    call_id = tool_call.get("id")
                    
                    try:
                        result = await self.skills.execute_tool_call(
                            tool_call,
                            user_id=context.get("user_id") if context else None,
                            chat_id=context.get("chat_id") if context else None,
                            platform=context.get("platform") if context else None
                        )
                        tool_output = result.output if result.success else f"Error: {result.error}"
                    except Exception as e:
                        tool_output = f"Execution failed: {str(e)}"
                    
                    messages.append({
                        "role": "tool",
                        "tool_call_id": call_id,
                        "name": function_name,
                        "content": str(tool_output)
                    })
                
                continue
                
            except Exception as e:
                logger.error(f"LLM interaction failed: {e}")
                return f"æŠ±æ­‰ï¼ŒAI æœåŠ¡æš‚æ—¶ä¸å¯ç”¨: {str(e)[:100]}"
        
        return "äº¤äº’æ¬¡æ•°è¿‡å¤šï¼Œå·²ç»ˆæ­¢ã€‚"
    
    async def _slow_handle_message_stream(
        self,
        text: str,
        context: dict = None,
        history: list = None,
        **kwargs
    ) -> AsyncGenerator[str, None]:
        """æ…¢é€Ÿæ¶ˆæ¯å¤„ç†å™¨ - æµå¼ç‰ˆæœ¬"""
        
        yield "â³ æ­£åœ¨æ€è€ƒ..."
        
        messages = []
        history = history or []
        
        # æ„å»ºç³»ç»Ÿæç¤º
        base_prompt = "ä½ æ˜¯ MLX-Agentï¼Œä¸€ä¸ªå¼ºå¤§çš„ AI åŠ©æ‰‹ã€‚"
        
        if self.identity:
            system_prompt = self.identity.inject_to_prompt(base_prompt)
        else:
            system_prompt = base_prompt
        
        # æœç´¢ç›¸å…³è®°å¿†
        if self.memory:
            try:
                memories = await self.memory.search(text, top_k=3)
                if memories:
                    memory_context = "\n\nç›¸å…³è®°å¿†:\n" + "\n".join([f"- {m.get('content', '')[:100]}" for m in memories[:3]])
                    system_prompt += memory_context
            except Exception as e:
                logger.warning(f"Memory search failed: {e}")
        
        messages.append({"role": "system", "content": system_prompt})
        
        # æ·»åŠ å†å²
        if history:
            history_to_use = [m for m in history if m.get("role") in ["user", "assistant", "tool"]][-20:]
            messages.extend(history_to_use)
        
        messages.append({"role": "user", "content": text})
        
        # æµå¼è°ƒç”¨ LLM
        if not self.llm:
            yield "LLM æœªé…ç½®"
            return
        
        try:
            buffer = ""
            async for chunk in self.llm.chat_stream(messages):
                if chunk["type"] == "content":
                    buffer += chunk["content"]
                    # ç´¯ç§¯ä¸€å®šé•¿åº¦æˆ–é‡åˆ°æ ‡ç‚¹å†è¾“å‡º
                    if len(buffer) > 20 or any(p in chunk["content"] for p in 'ã€‚ï¼ï¼Ÿ\n'):
                        yield buffer
                        buffer = ""
                elif chunk["type"] == "done":
                    if buffer:
                        yield buffer
                    break
                elif chunk["type"] == "error":
                    yield f"\n[é”™è¯¯: {chunk.get('error', 'unknown')}]"
                    break
        except Exception as e:
            logger.error(f"Stream handling failed: {e}")
            yield f"æŠ±æ­‰ï¼Œæµå¼è¾“å‡ºå¤±è´¥: {str(e)[:100]}"
    
    async def _on_task_complete(self, task: Task, result: TaskResult):
        """ä»»åŠ¡å®Œæˆå›è°ƒ"""
        logger.info(f"Task {task.id} completed, notifying user {task.user_id}")
        
        # æ„å»ºæ¶ˆæ¯
        if task.type == "chat" and result.success:
            message = str(result.output)
        else:
            if result.success:
                icon = "âœ…"
                status = "å®Œæˆ"
            else:
                icon = "âŒ"
                status = "å¤±è´¥"
            
            message = (
                f"{icon} ä»»åŠ¡ `{task.id}` {status}\n"
                f"â±ï¸ è€—æ—¶: {result.duration_ms/1000:.1f}s\n"
            )
            
            if result.output:
                output_text = str(result.output)
                if len(output_text) > 500:
                    output_text = output_text[:500] + "..."
                message += f"\nğŸ“¤ ç»“æœ:\n{output_text}"
            
            if result.error:
                error_text = str(result.error)
                if len(error_text) > 200:
                    error_text = error_text[:200] + "..."
                message += f"\nâ— é”™è¯¯: {error_text}"
        
        # å‘é€åˆ°å¹³å°
        if task.platform == "telegram" and self.telegram:
            try:
                await self.telegram.send_message(task.chat_id, message)
            except Exception as e:
                logger.error(f"Failed to send task notification: {e}")
            finally:
                await self.telegram.stop_typing_loop(task.chat_id)
        
        # ä¿å­˜åˆ°è®°å¿†
        if self.memory and result.success:
            try:
                await self.memory.add(
                    f"Task {task.id} completed: {result.output[:200] if result.output else 'No output'}",
                    metadata={
                        'platform': task.platform,
                        'user_id': task.user_id,
                        'task_type': task.type,
                        'task_id': task.id
                    },
                    level='P2'
                )
            except Exception as e:
                logger.warning(f"Failed to save task memory: {e}")
    
    async def handle_message(
        self,
        platform: str,
        user_id: str,
        text: str,
        chat_id: str = None,
        message_id: str = None,
        username: str = None
    ) -> str:
        """å¤„ç†ç”¨æˆ·æ¶ˆæ¯"""
        logger.debug(f"[AGENT] handle_message: platform={platform}, user_id={user_id}, text={text[:50]}...")
        
        try:
            if self.identity:
                await self.identity.check_reload()
            
            if self.chat_manager:
                session = self.chat_manager.get_or_create(
                    platform=platform,
                    user_id=user_id,
                    chat_id=chat_id or user_id,
                    message_id=message_id,
                    username=username,
                    notify_callback=self._create_notify_callback(platform, chat_id or user_id)
                )
                
                response = await session.handle_message(text)
                
                if response.is_task:
                    if platform == "telegram" and self.telegram and (chat_id or user_id):
                        await self.telegram.start_typing_loop(chat_id or user_id)
                    return None
                
                return response.text
            
            return await self._legacy_handle_message(platform, user_id, text)
            
        except Exception as e:
            logger.error(f"[AGENT ERROR] {type(e).__name__}: {e}")
            logger.exception("[AGENT ERROR] Full traceback:")
            return f"âŒ å¤„ç†æ¶ˆæ¯æ—¶å‡ºé”™: {e}"
    
    async def handle_message_stream(
        self,
        platform: str,
        user_id: str,
        text: str,
        chat_id: str = None,
        message_id: str = None
    ) -> AsyncGenerator[str, None]:
        """æµå¼å¤„ç†ç”¨æˆ·æ¶ˆæ¯"""
        # ä½¿ç”¨æ–°çš„æµå¼å¤„ç†å™¨
        async for chunk in self._slow_handle_message_stream(
            text=text,
            context={"platform": platform, "user_id": user_id, "chat_id": chat_id},
            history=[]
        ):
            yield chunk
    
    def _create_notify_callback(self, platform: str, chat_id: str):
        """åˆ›å»ºé€šçŸ¥å›è°ƒå‡½æ•°"""
        async def notify_callback(task: Task, result: TaskResult):
            pass
        return notify_callback
    
    async def _legacy_handle_message(self, platform: str, user_id: str, text: str) -> str:
        """ä¼ ç»Ÿçš„æ¶ˆæ¯å¤„ç†æ–¹å¼ï¼ˆé™çº§æ–¹æ¡ˆï¼‰"""
        try:
            memories = await self.memory.search(text, top_k=5) if self.memory else []
            memory_context = self._format_memories(memories[:3])
            return f"æ”¶åˆ°: {text}\n\nç›¸å…³è®°å¿†:\n{memory_context or '(æ— )'}"
        except Exception as e:
            return f"å¤„ç†å¤±è´¥: {e}"
    
    def _format_memories(self, memories: list) -> str:
        """æ ¼å¼åŒ–è®°å¿†ä¸ºä¸Šä¸‹æ–‡"""
        if not memories:
            return ""
        return "\n".join(f"- {m['content'][:200]}" for m in memories)
    
    async def _scheduled_tasks(self):
        """å®šæ—¶ä»»åŠ¡"""
        while self._running:
            try:
                # æ¯å°æ—¶æ£€æŸ¥ä¸€æ¬¡äººè®¾æ–‡ä»¶çƒ­é‡è½½
                await asyncio.wait_for(self._shutdown_event.wait(), timeout=3600)
                if not self._running:
                    break
                    
                if self.identity:
                    await self.identity.check_reload()
                
                # æ¯å¤©æ‰§è¡Œè®°å¿†æ•´åˆ
                now = time.time()
                if hasattr(self, '_last_consolidation'):
                    if now - self._last_consolidation > 86400:
                        await self._run_consolidation()
                else:
                    self._last_consolidation = now
                    
            except asyncio.TimeoutError:
                continue
            except Exception as e:
                logger.error(f"Scheduled task error: {e}")
    
    async def _run_consolidation(self):
        """è¿è¡Œè®°å¿†æ•´åˆ"""
        if not self.consolidator:
            return
        
        logger.info("Running memory consolidation...")
        report = await self.consolidator.consolidate(days_back=7, dry_run=False)
        logger.info(f"Consolidation report: {report}")
        self._last_consolidation = time.time()
    
    async def get_stats(self) -> dict:
        """è·å– Agent ç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            'version': self.config.version,
            'running': self._running,
            'identity': self.identity.get_identity_summary() if self.identity else None,
            'skills': {
                'native': len(self.skills.skills) if self.skills else 0,
                'openclaw': len(self.openclaw_skills.skills) if self.openclaw_skills else 0
            },
            'memory': self.memory.get_stats() if self.memory else None,
            'tasks': self.task_queue.get_stats() if self.task_queue else None,
            'worker': self.task_worker.get_stats() if self.task_worker else None,
            'sessions': self.chat_manager.get_stats() if self.chat_manager else None
        }
        return stats
