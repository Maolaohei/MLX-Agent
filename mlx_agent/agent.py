"""
MLX-Agent ä¸»ç±»

æ ¸å¿ƒåŠŸèƒ½ï¼š
- è®°å¿†ç³»ç»Ÿç®¡ç†
- å¹³å°é€‚é…å™¨ç®¡ç†
- Skill ç³»ç»Ÿç®¡ç†
- LLM è·¯ç”±
- äººè®¾ç®¡ç†
- Token å‹ç¼©
- å¼‚æ­¥ä»»åŠ¡é˜Ÿåˆ—
"""

import asyncio
import signal
import time
from pathlib import Path
from typing import Optional, Dict, Any, AsyncGenerator

import uvloop
from loguru import logger

from .config import Config
from .memory import MemorySystem
from .memory.consolidation import MemoryConsolidator
from .identity import IdentityManager
from .compression import TokenCompressor
from .skills import SkillRegistry
from .skills.compat.openclaw import OpenClawSkillAdapter
from .tasks import TaskQueue, TaskWorker, TaskExecutor, TaskPriority, Task, TaskResult
from .chat import ChatSessionManager, ChatResponse
from .llm import LLMClient
from .api_manager import APIManager, get_api_manager


class MLXAgent:
    """MLX-Agent ä¸»ç±»
    
    é«˜æ€§èƒ½ã€è½»é‡çº§ã€å¤šå¹³å° AI Agent
    å¸¦æœ‰äººè®¾å®ˆæŠ¤ã€Token å‹ç¼©ã€è®°å¿†æ•´åˆç­‰é«˜çº§ç‰¹æ€§
    
    Example:
        >>> agent = MLXAgent(config_path="config.yaml")
        >>> await agent.start()
    """
    
    def __init__(self, config_path: Optional[str] = None):
        """åˆå§‹åŒ– MLX-Agent
        
        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„ï¼Œé»˜è®¤ä½¿ç”¨ config/config.yaml
        """
        # ä½¿ç”¨ uvloop åŠ é€Ÿ asyncio (å¦‚æœæ”¯æŒ)
        try:
            import uvloop
            asyncio.set_event_loop_policy(uvloop.EventLoopPolicy())
        except ImportError:
            pass
        
        # åˆ›å»ºæ–°çš„äº‹ä»¶å¾ªç¯
        try:
            self.loop = asyncio.get_event_loop()
            if self.loop.is_closed():
                self.loop = asyncio.new_event_loop()
                asyncio.set_event_loop(self.loop)
        except RuntimeError:
            self.loop = asyncio.new_event_loop()
            asyncio.set_event_loop(self.loop)
        
        # åŠ è½½é…ç½®
        self.config = Config.load(config_path)
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.memory: Optional[MemorySystem] = None
        self.consolidator: Optional[MemoryConsolidator] = None
        self.identity: Optional[IdentityManager] = None
        self.compressor: Optional[TokenCompressor] = None
        self.skills: Optional[SkillRegistry] = None
        self.openclaw_skills: Optional[OpenClawSkillAdapter] = None
        self.telegram: Optional[TelegramAdapter] = None
        self.api_manager: Optional[APIManager] = None  # API ç®¡ç†å™¨
        self._running = False
        
        # ä»»åŠ¡ç³»ç»Ÿ
        self.task_queue: Optional[TaskQueue] = None
        self.task_executor: Optional[TaskExecutor] = None
        self.task_worker: Optional[TaskWorker] = None
        self.chat_manager: Optional[ChatSessionManager] = None
        
        # LLM å®¢æˆ·ç«¯
        self.llm: Optional[LLMClient] = None
        
        # è®¾ç½®ä¿¡å·å¤„ç†
        self._setup_signal_handlers()
        
        logger.info(f"MLX-Agent v{self.config.version} initialized")
    
    def _setup_signal_handlers(self):
        """è®¾ç½®ä¿¡å·å¤„ç†å™¨"""
        for sig in (signal.SIGTERM, signal.SIGINT):
            self.loop.add_signal_handler(sig, lambda: asyncio.create_task(self.stop()))
    
    async def start(self):
        """å¯åŠ¨ Agent"""
        if self._running:
            logger.warning("Agent already running")
            return
        
        logger.info("Starting MLX-Agent...")
        self._running = True
        
        try:
            # 1. åˆå§‹åŒ–äººè®¾ç®¡ç†å™¨ï¼ˆæœ€å…ˆåŠ è½½ï¼Œç¡®ä¿çŸ¥é“è‡ªå·±æ˜¯è°ï¼‰
            self.identity = IdentityManager(Path(self.config.memory.path).parent)
            await self.identity.load()
            logger.info(f"Identity loaded: {self.identity.get_identity_summary()}")
            
            # 2. åˆå§‹åŒ– Token å‹ç¼©å™¨
            self.compressor = TokenCompressor(model=self.config.llm.model)
            logger.info("Token compressor initialized")
            
            # 3. åˆå§‹åŒ– API ç®¡ç†å™¨ï¼ˆå¿…é¡»åœ¨æŠ€èƒ½ç³»ç»Ÿä¹‹å‰ï¼‰
            self.api_manager = get_api_manager()
            await self.api_manager.initialize()
            logger.info("API manager initialized")
            
            # 4. åˆå§‹åŒ–è®°å¿†ç³»ç»Ÿ
            self.memory = MemorySystem(self.config.memory)
            await self.memory.initialize()
            logger.info("Memory system initialized")
            
            # 4. åˆå§‹åŒ–è®°å¿†æ•´åˆå™¨
            self.consolidator = MemoryConsolidator(
                Path(self.config.memory.path),
                similarity_threshold=0.7
            )
            logger.info("Memory consolidator initialized")
            
            # 5. åˆå§‹åŒ– LLM å®¢æˆ·ç«¯
            try:
                # å°è¯•ä½¿ç”¨æ–°é…ç½®ç»“æ„
                primary_config = None
                fallback_config = None
                failover_enabled = False
                
                # æ£€æŸ¥ config.llm æ˜¯å¦æ˜¯ Pydantic å¯¹è±¡ä¸”æœ‰ primary å­—æ®µ
                if hasattr(self.config.llm, 'primary') and self.config.llm.primary:
                    logger.info("Using multi-model configuration")
                    primary_data = self.config.llm.primary
                    primary_config = {
                        'api_key': primary_data.api_key,
                        'api_base': primary_data.api_base,
                        'auth_token': primary_data.auth_token,
                        'model': primary_data.model,
                        'temperature': primary_data.temperature,
                        'max_tokens': primary_data.max_tokens,
                    }
                    
                    if self.config.llm.fallback:
                        fallback_data = self.config.llm.fallback
                        fallback_config = {
                            'api_key': fallback_data.api_key,
                            'api_base': fallback_data.api_base,
                            'auth_token': fallback_data.auth_token,
                            'model': fallback_data.model,
                            'temperature': fallback_data.temperature,
                            'max_tokens': fallback_data.max_tokens,
                        }
                    
                    failover_enabled = self.config.llm.failover.enabled
                else:
                    # å…¼å®¹æ—§é…ç½®
                    logger.info("Using legacy LLM configuration")
                    primary_config = {
                        'api_key': self.config.llm.api_key,
                        'api_base': self.config.llm.api_base,
                        'auth_token': self.config.llm.auth_token,
                        'model': self.config.llm.model,
                        'temperature': self.config.llm.temperature,
                        'max_tokens': self.config.llm.max_tokens,
                    }
                
                if primary_config and primary_config.get('api_key'):
                    self.llm = LLMClient(
                        primary_config=primary_config,
                        fallback_config=fallback_config,
                        failover_enabled=failover_enabled
                    )
                    logger.info(f"LLM client initialized: {primary_config.get('model')}")
                else:
                    logger.error("LLM config missing API Key")
                    
            except Exception as e:
                logger.error(f"Failed to initialize LLM: {e}")
                import traceback
                logger.error(traceback.format_exc())
            
            # 6. åˆå§‹åŒ– Skill ç³»ç»Ÿ
            self.skills = SkillRegistry(self)
            await self.skills.initialize()
            logger.info("Skill system initialized")
            
            # 6. åˆå§‹åŒ– OpenClaw å…¼å®¹å±‚
            self.openclaw_skills = OpenClawSkillAdapter()
            await self.openclaw_skills.initialize()
            oc_skills = self.openclaw_skills.list_skills()
            logger.info(f"OpenClaw adapter initialized with {len(oc_skills)} skills")
            
            # 7. åˆå§‹åŒ–ä»»åŠ¡ç³»ç»Ÿ
            await self._init_task_system()
            logger.info("Task system initialized")
            
            # 8. åˆå§‹åŒ–å¹³å°é€‚é…å™¨
            if self.config.platforms.telegram.enabled:
                from .platforms.telegram import TelegramAdapter
                self.telegram = TelegramAdapter(
                    self.config.platforms.telegram,
                    self
                )
                await self.telegram.initialize()
                # åœ¨åå°å¯åŠ¨ Telegram
                asyncio.create_task(self.telegram.start())
                logger.info("Telegram adapter started")
            
            logger.info("MLX-Agent started successfully!")
            
            # 8. å¯åŠ¨å®šæ—¶ä»»åŠ¡
            asyncio.create_task(self._scheduled_tasks())
            
            # ä¿æŒè¿è¡Œ
            while self._running:
                await asyncio.sleep(1)
                
        except Exception as e:
            logger.error(f"Failed to start agent: {e}")
            await self.stop()
            raise
    
    async def stop(self):
        """åœæ­¢ Agent"""
        if not self._running:
            return
        
        logger.info("Stopping MLX-Agent...")
        self._running = False
        
        # åœæ­¢ä»»åŠ¡ç³»ç»Ÿ
        if self.task_worker:
            await self.task_worker.stop()
        
        if self.task_executor:
            self.task_executor.shutdown()
        
        if self.task_queue:
            await self.task_queue.shutdown()
        
        # æ¸…ç†èµ„æº
        if self.memory:
            await self.memory.close()
        
        if self.skills:
            await self.skills.close()
        
        if self.telegram:
            await self.telegram.stop()
        
        logger.info("MLX-Agent stopped")
    
    async def _scheduled_tasks(self):
        """å®šæ—¶ä»»åŠ¡"""
        while self._running:
            try:
                # æ¯å°æ—¶æ£€æŸ¥ä¸€æ¬¡äººè®¾æ–‡ä»¶çƒ­é‡è½½
                await asyncio.sleep(3600)
                if self.identity:
                    await self.identity.check_reload()
                
                # æ¯å¤©å‡Œæ™¨ 2 ç‚¹æ‰§è¡Œè®°å¿†æ•´åˆ
                now = asyncio.get_event_loop().time()
                # ç®€åŒ–ï¼šæ¯24å°æ—¶æ•´åˆä¸€æ¬¡
                if hasattr(self, '_last_consolidation'):
                    if now - self._last_consolidation > 86400:
                        await self._run_consolidation()
                else:
                    self._last_consolidation = now
                    
            except Exception as e:
                logger.error(f"Scheduled task error: {e}")
    
    async def _run_consolidation(self):
        """è¿è¡Œè®°å¿†æ•´åˆ"""
        if not self.consolidator:
            return
        
        logger.info("Running memory consolidation...")
        report = await self.consolidator.consolidate(days_back=7, dry_run=False)
        logger.info(f"Consolidation report: {report}")
        self._last_consolidation = asyncio.get_event_loop().time()
    
    async def _init_task_system(self):
        """åˆå§‹åŒ–ä»»åŠ¡ç³»ç»Ÿ"""
        # åˆ›å»ºä»»åŠ¡é˜Ÿåˆ—
        self.task_queue = TaskQueue(maxsize=1000)
        
        # åˆ›å»ºæ‰§è¡Œå™¨ï¼ˆçº¿ç¨‹æ± ï¼‰
        self.task_executor = TaskExecutor(max_workers=4)
        
        # åˆ›å»ºå·¥ä½œçº¿ç¨‹
        self.task_worker = TaskWorker(
            queue=self.task_queue,
            executor=self.task_executor,
            num_workers=2,
            default_callback=self._on_task_complete
        )
        
        # å¯åŠ¨å·¥ä½œçº¿ç¨‹
        await self.task_worker.start()
        
        # åˆ›å»ºèŠå¤©ä¼šè¯ç®¡ç†å™¨
        self.chat_manager = ChatSessionManager(
            task_queue=self.task_queue,
            quick_handler=self._quick_handle_message,
            slow_handler=self._slow_handle_message
        )
    
    async def _quick_handle_message(self, text: str, context: dict = None, history: list = None, **kwargs) -> Optional[str]:
        """å¿«é€Ÿæ¶ˆæ¯å¤„ç†å™¨
        
        å¤„ç†ç®€å•ã€å¿«é€Ÿå“åº”çš„è¯·æ±‚
        
        Args:
            text: ç”¨æˆ·æ¶ˆæ¯
            context: ä¸Šä¸‹æ–‡ä¿¡æ¯
            history: å¯¹è¯å†å²
            
        Returns:
            å“åº”æ–‡æœ¬ï¼ŒNone è¡¨ç¤ºéœ€è¦è½¬å…¥æ…¢é€Ÿå¤„ç†
        """
        # ç®€å•å‘½ä»¤å¤„ç†
        text_lower = text.lower().strip()
        
        # å¸®åŠ©å‘½ä»¤
        if text_lower in ['/help', 'help', 'å¸®åŠ©']:
            return (
                "ğŸ¤– MLX-Agent å¸®åŠ©\n\n"
                "ğŸ’¬ å¿«é€Ÿå“åº”:\n"
                "â€¢ /help - æ˜¾ç¤ºå¸®åŠ©\n"
                "â€¢ /status - æŸ¥çœ‹çŠ¶æ€\n"
                "â€¢ /tasks - æŸ¥çœ‹è¿›è¡Œä¸­çš„ä»»åŠ¡\n\n"
                "â³ æ…¢é€Ÿä»»åŠ¡ä¼šè‡ªåŠ¨è¿›å…¥é˜Ÿåˆ—ï¼Œå®Œæˆåé€šçŸ¥ä½ ~"
            )
        
        # çŠ¶æ€å‘½ä»¤
        if text_lower in ['/status', 'status', 'çŠ¶æ€']:
            stats = await self.get_stats()
            queue_stats = self.task_queue.get_stats() if self.task_queue else {}
            return (
                f"ğŸ“Š çŠ¶æ€\n"
                f"â€¢ Agent: {'è¿è¡Œä¸­' if stats['running'] else 'å·²åœæ­¢'}\n"
                f"â€¢ ä»»åŠ¡é˜Ÿåˆ—: {queue_stats.get('pending', 0)} ç­‰å¾… / "
                f"{queue_stats.get('running', 0)} æ‰§è¡Œä¸­\n"
                f"â€¢ Skills: {stats['skills']['native']} åŸç”Ÿ / "
                f"{stats['skills']['openclaw']} OpenClaw"
            )
        
        # ä»»åŠ¡åˆ—è¡¨å‘½ä»¤
        if text_lower in ['/tasks', 'tasks', 'ä»»åŠ¡']:
            if context and self.task_queue:
                user_id = context.get('user_id')
                tasks = self.task_queue.get_user_tasks(user_id)
                if tasks:
                    task_list = "\n".join([
                        f"â€¢ {t.id}: {t.status.value} ({t.type})"
                        for t in tasks[:10]
                    ])
                    return f"ğŸ“‹ ä½ çš„ä»»åŠ¡ ({len(tasks)}):\n{task_list}"
                return "ğŸ“‹ å½“å‰æ²¡æœ‰è¿›è¡Œä¸­çš„ä»»åŠ¡"
        
        # ç®€å•çš„é—®å€™è¯­
        greetings = ['hello', 'hi', 'ä½ å¥½', 'æ‚¨å¥½', 'åœ¨å—', 'åœ¨ï¼Ÿ']
        if any(g in text_lower for g in greetings):
            return "ğŸ‘‹ ä½ å¥½ï¼æˆ‘æ˜¯ MLX-Agentï¼Œæœ‰ä»€ä¹ˆå¯ä»¥å¸®ä½ çš„å—ï¼Ÿ"
        
        # çŸ­æ¶ˆæ¯ä¹Ÿä½¿ç”¨ LLM å›å¤ï¼ˆä¸æ˜¯å¤è¯»æœºï¼‰
        if len(text) < 50 and self.llm:
            # ä½¿ç”¨ LLM ç”Ÿæˆå›å¤ï¼Œä¸ç»è¿‡æ…¢é€Ÿé˜Ÿåˆ—
            try:
                base_prompt = "ç®€çŸ­å›å¤ã€‚"
                if self.identity:
                    system_prompt = self.identity.inject_to_prompt(base_prompt)
                else:
                    system_prompt = base_prompt
                
                response = await self.llm.simple_chat(text, system_prompt)
                return response
            except Exception as e:
                logger.error(f"Quick LLM call failed: {e}")
                # å¦‚æœ LLM å¤±è´¥ï¼Œè½¬å…¥æ…¢é€Ÿé˜Ÿåˆ—
                return None
        
        # éœ€è¦å¤æ‚å¤„ç†çš„è¿”å› Noneï¼Œè½¬å…¥æ…¢é€Ÿé˜Ÿåˆ—
        return None
    
    async def _slow_handle_message(
        self,
        text: str,
        context: dict = None,
        task: Task = None,
        history: list = None,
        **kwargs
    ) -> str:
        """æ…¢é€Ÿæ¶ˆæ¯å¤„ç†å™¨ - ä½¿ç”¨ LLM ç”Ÿæˆæ™ºèƒ½å›å¤ (æ”¯æŒå·¥å…·è°ƒç”¨å’Œå¯¹è¯å†å²)"""
        
        if task:
            task.set_progress("ğŸ¤” æ­£åœ¨ç†è§£ä½ çš„é—®é¢˜...", 0.1)
        
        # 0. å‡†å¤‡å¯¹è¯å†å²
        messages = []
        history = history or []
        
        # 1. å‡†å¤‡ä¸Šä¸‹æ–‡å’Œç³»ç»Ÿæç¤º
        # æœç´¢ç›¸å…³è®°å¿† (ä½œä¸º System Prompt çš„è¡¥å……)
        memories = []
        if self.memory:
            try:
                memories = await self.memory.search(text, top_k=3)
                if task:
                    task.set_progress("ğŸ” æœç´¢ç›¸å…³è®°å¿†...", 0.3)
            except Exception as e:
                logger.warning(f"Memory search failed: {e}")
        
        # æ„å»ºåŸºç¡€ Prompt
        base_prompt = "ä½ æ˜¯ MLX-Agentï¼Œä¸€ä¸ªå¼ºå¤§çš„ AI åŠ©æ‰‹ã€‚è¯·ä¿æŒå¯¹è¯è¿è´¯æ€§ï¼Œå‚è€ƒä¹‹å‰çš„å¯¹è¯å†å²ã€‚"
        
        # ä½¿ç”¨ IdentityManager ç”Ÿæˆå®Œæ•´ Prompt
        if self.identity:
            system_prompt = self.identity.inject_to_prompt(base_prompt)
        else:
            system_prompt = base_prompt
            
        # è¡¥å……æ¨¡å‹ä¿¡æ¯
        current_model = "unknown"
        if self.llm:
            current_model = self.llm.get_current_model()
            system_prompt += f"\n\nå½“å‰ä½¿ç”¨çš„æ¨¡å‹: {current_model}"
            
            # å¦‚æœæ˜¯ Gemini-3 Proï¼Œå¢åŠ ç‰¹å®šæŒ‡ä»¤
            if "gemini-3" in current_model:
                system_prompt += "\n\nè¯·å……åˆ†åˆ©ç”¨ Gemini-3 Pro çš„æ¨ç†èƒ½åŠ›ï¼Œå›ç­”è¦æ·±å…¥ã€å…¨é¢ã€‚"

        # å¦‚æœæœ‰è®°å¿†ï¼Œæ·»åŠ åˆ°ç³»ç»Ÿæç¤º
        if memories:
            memory_context = "\n\nç›¸å…³è®°å¿†:\n" + "\n".join([f"- {m.get('content', '')[:100]}" for m in memories[:3]])
            system_prompt += memory_context
        
        # æ„å»ºæ¶ˆæ¯åˆ—è¡¨ï¼šç³»ç»Ÿæç¤º + å†å² + å½“å‰æ¶ˆæ¯
        messages.append({"role": "system", "content": system_prompt})
        
        # æ·»åŠ å†å²å¯¹è¯ï¼ˆæœ€å¤šä¿ç•™æœ€è¿‘10è½®ï¼Œé¿å…è¶…å‡ºä¸Šä¸‹æ–‡é™åˆ¶ï¼‰
        if history:
            # è¿‡æ»¤æ‰ system æ¶ˆæ¯ï¼Œåªä¿ç•™ user/assistant/tool
            history_to_use = [m for m in history if m.get("role") in ["user", "assistant", "tool"]][-20:]
            messages.extend(history_to_use)
            logger.debug(f"[LLM] Using {len(history_to_use)} history messages")
        
        # æ·»åŠ å½“å‰ç”¨æˆ·æ¶ˆæ¯
        messages.append({"role": "user", "content": text})
        
        # 2. è·å–å¯ç”¨å·¥å…·
        tools = None
        if self.skills:
            try:
                tools = self.skills.get_tools_schema()
                if tools:
                    logger.debug(f"Available tools: {len(tools)}")
            except Exception as e:
                logger.error(f"Failed to get tools: {e}")
        
        if task:
            task.set_progress("ğŸ§  è°ƒç”¨ AI ç”Ÿæˆå›å¤...", 0.6)
        
        # 3. LLM äº¤äº’å¾ªç¯ (æ”¯æŒå¤šè½®å·¥å…·è°ƒç”¨)
        max_turns = 5  # é˜²æ­¢æ— é™å¾ªç¯
        turn_count = 0
        
        while turn_count < max_turns:
            turn_count += 1
            
            if not self.llm:
                return f"æ”¶åˆ°ä½ çš„æ¶ˆæ¯: {text[:100]}...\n\nï¼ˆLLM æœªé…ç½®ï¼Œæ— æ³•ç”Ÿæˆæ™ºèƒ½å›å¤ï¼‰"
                
            try:
                # è°ƒç”¨ LLM
                # å¦‚æœæä¾›äº†å·¥å…·ï¼Œå¯ç”¨æ€è€ƒæ¨¡å¼ï¼ˆKimi k2.5 æ”¯æŒï¼‰
                use_reasoning = tools is not None and len(tools) > 0
                
                response_msg = await self.llm.chat(
                    messages=messages,
                    tools=tools,
                    tool_choice="auto" if tools else None,
                    reasoning=use_reasoning
                )
                
                # æ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨
                tool_calls = response_msg.get("tool_calls")
                content = response_msg.get("content")
                
                # å¦‚æœæœ‰å†…å®¹ï¼Œå…ˆæ·»åŠ åˆ°å†å² (Assistant Message)
                # æ³¨æ„ï¼šæœ‰äº›æ¨¡å‹å¯èƒ½åŒæ—¶è¿”å› content å’Œ tool_calls
                # OpenAI è§„èŒƒè¦æ±‚ Assistant Message å¿…é¡»åŒ…å« tool_calls å­—æ®µå¦‚æœå®ƒè¢«ä½¿ç”¨äº†
                assistant_msg = {
                    "role": "assistant",
                    "content": content
                }
                if tool_calls:
                    assistant_msg["tool_calls"] = tool_calls
                
                messages.append(assistant_msg)
                
                if not tool_calls:
                    # æ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œç›´æ¥è¿”å›å†…å®¹
                    if task:
                        task.set_progress("âœ¨ å®Œæˆ", 1.0)
                    return content or "ï¼ˆAI æœªè¿”å›ä»»ä½•å†…å®¹ï¼‰"
                
                # æœ‰å·¥å…·è°ƒç”¨ï¼Œæ‰§è¡Œå·¥å…·
                if task:
                    task.set_progress(f"ğŸ”§ æ‰§è¡Œå·¥å…· ({len(tool_calls)} ä¸ª)...", 0.8)
                
                for tool_call in tool_calls:
                    function_name = tool_call.get("function", {}).get("name")
                    call_id = tool_call.get("id")
                    
                    # æ‰§è¡Œå·¥å…·
                    try:
                        result = await self.skills.execute_tool_call(
                            tool_call,
                            user_id=context.get("user_id") if context else None,
                            chat_id=context.get("chat_id") if context else None,
                            platform=context.get("platform") if context else None
                        )
                        
                        tool_output = result.output if result.success else f"Error: {result.error}"
                        
                    except Exception as e:
                        tool_output = f"Execution failed: {str(e)}"
                    
                    # æ·»åŠ å·¥å…·æ‰§è¡Œç»“æœ (Tool Message)
                    messages.append({
                        "role": "tool",
                        "tool_call_id": call_id,
                        "name": function_name,
                        "content": str(tool_output)
                    })
                
                # ç»§ç»­ä¸‹ä¸€è½®å¾ªç¯ï¼Œå°†å·¥å…·ç»“æœä¼ å› LLM
                continue
                
            except Exception as e:
                logger.error(f"LLM interaction failed: {e}")
                return f"æŠ±æ­‰ï¼ŒAI æœåŠ¡æš‚æ—¶ä¸å¯ç”¨: {str(e)[:100]}"
        
        return "äº¤äº’æ¬¡æ•°è¿‡å¤šï¼Œå·²ç»ˆæ­¢ã€‚"
    
    async def _on_task_complete(self, task: Task, result: TaskResult):
        """ä»»åŠ¡å®Œæˆå›è°ƒ
        
        ä¸»åŠ¨æ¨é€ç»“æœç»™ç”¨æˆ·
        """
        logger.info(f"Task {task.id} completed, notifying user {task.user_id}")
        
        # é’ˆå¯¹èŠå¤©ä»»åŠ¡çš„ç‰¹æ®Šå¤„ç†ï¼šåªå‘é€ç»“æœï¼Œä¸å‘é€çŠ¶æ€å¤´
        if task.type == "chat" and result.success:
            message = str(result.output)
        else:
            # å…¶ä»–ä»»åŠ¡æˆ–å¤±è´¥æ—¶ï¼Œä¿ç•™è¯¦ç»†ä¿¡æ¯
            if result.success:
                icon = "âœ…"
                status = "å®Œæˆ"
            else:
                icon = "âŒ"
                status = "å¤±è´¥"
            
            message = (
                f"{icon} ä»»åŠ¡ `{task.id}` {status}\n"
                f"â±ï¸ è€—æ—¶: {result.duration_ms/1000:.1f}s\n"
            )
            
            if result.output:
                output_text = str(result.output)
                if len(output_text) > 500:
                    output_text = output_text[:500] + "..."
                message += f"\nğŸ“¤ ç»“æœ:\n{output_text}"
            
            if result.error:
                error_text = str(result.error)
                if len(error_text) > 200:
                    error_text = error_text[:200] + "..."
                message += f"\nâ— é”™è¯¯: {error_text}"
        
        # å‘é€åˆ°å¹³å°
        if task.platform == "telegram" and self.telegram:
            try:
                # å…ˆå‘æ¶ˆæ¯
                await self.telegram.send_message(task.chat_id, message)
                logger.debug(f"Task notification sent to {task.chat_id}")
            except Exception as e:
                logger.error(f"Failed to send task notification: {e}")
            finally:
                # æ— è®ºæˆåŠŸå¤±è´¥ï¼Œå‘å®Œæ¶ˆæ¯åæ‰åœæ­¢ Typing
                await self.telegram.stop_typing_loop(task.chat_id)
        
        # ä¿å­˜åˆ°è®°å¿†
        if self.memory and result.success:
            try:
                await self.memory.add(
                    f"Task {task.id} completed: {result.output[:200] if result.output else 'No output'}",
                    metadata={
                        'platform': task.platform,
                        'user_id': task.user_id,
                        'task_type': task.type,
                        'task_id': task.id
                    },
                    level='P2'
                )
            except Exception as e:
                logger.warning(f"Failed to save task memory: {e}")
    
    async def handle_message(
        self,
        platform: str,
        user_id: str,
        text: str,
        chat_id: str = None,
        message_id: str = None,
        username: str = None
    ) -> str:
        """å¤„ç†ç”¨æˆ·æ¶ˆæ¯
        
        è‡ªåŠ¨åˆ¤æ–­æ˜¯å¿«é€Ÿå“åº”è¿˜æ˜¯æ…¢é€Ÿä»»åŠ¡ï¼š
        - å¿«é€Ÿå“åº”ï¼šç›´æ¥è¿”å›ï¼ˆ<100msï¼‰
        - æ…¢é€Ÿä»»åŠ¡ï¼šè¿›å…¥é˜Ÿåˆ—å¼‚æ­¥å¤„ç†ï¼Œç«‹å³è¿”å›ä»»åŠ¡ID
        """
        logger.debug(f"[AGENT] handle_message called: platform={platform}, user_id={user_id}, text={text[:50]}...")
        
        try:
            # 1. æ£€æŸ¥äººè®¾çƒ­é‡è½½
            if self.identity:
                await self.identity.check_reload()
            
            # 2. ä½¿ç”¨èŠå¤©ä¼šè¯ç®¡ç†å™¨å¤„ç†
            if self.chat_manager:
                logger.debug("[AGENT] Using chat_manager")
                session = self.chat_manager.get_or_create(
                    platform=platform,
                    user_id=user_id,
                    chat_id=chat_id or user_id,
                    message_id=message_id,
                    username=username,
                    notify_callback=self._create_notify_callback(platform, chat_id or user_id)
                )
                
                logger.debug("[AGENT] Calling session.handle_message")
                response = await session.handle_message(text)
                logger.debug(f"[AGENT] Got response: {response.text[:100] if response and response.text else 'None'}...")
                
                # å¦‚æœæ˜¯ä»»åŠ¡åˆ›å»ºï¼Œå¯åŠ¨æ‰“å­—çŠ¶æ€å¹¶è¿”å›ç©ºï¼ˆè®© adapter ä¿æŒæ²‰é»˜ï¼‰
                if response.is_task:
                    if platform == "telegram" and self.telegram and (chat_id or user_id):
                        await self.telegram.start_typing_loop(chat_id or user_id)
                    return None
                    
                return response.text
            
            # 3. é™çº§ï¼šç›´æ¥å¤„ç†ï¼ˆæ— ä»»åŠ¡ç³»ç»Ÿï¼‰
            logger.debug("[AGENT] Using legacy handler")
            return await self._legacy_handle_message(platform, user_id, text)
            
        except Exception as e:
            logger.error(f"[AGENT ERROR] {type(e).__name__}: {e}")
            logger.exception("[AGENT ERROR] Full traceback:")
            return f"âŒ å¤„ç†æ¶ˆæ¯æ—¶å‡ºé”™: {e}"
    
    def _create_notify_callback(self, platform: str, chat_id: str):
        """åˆ›å»ºé€šçŸ¥å›è°ƒå‡½æ•°"""
        async def notify_callback(task: Task, result: TaskResult):
            # è¿™é‡Œå¯ä»¥æ·»åŠ é¢å¤–çš„é€šçŸ¥é€»è¾‘
            pass
        return notify_callback
    
    async def _legacy_handle_message(self, platform: str, user_id: str, text: str) -> str:
        """ä¼ ç»Ÿçš„æ¶ˆæ¯å¤„ç†æ–¹å¼ï¼ˆé™çº§æ–¹æ¡ˆï¼‰"""
        try:
            memories = await self.memory.search(text, top_k=5) if self.memory else []
            memory_context = self._format_memories(memories[:3])
            return f"æ”¶åˆ°: {text}\n\nç›¸å…³è®°å¿†:\n{memory_context or '(æ— )'}"
        except Exception as e:
            return f"å¤„ç†å¤±è´¥: {e}"
    
    async def handle_message_stream(
        self,
        platform: str,
        user_id: str,
        text: str,
        chat_id: str = None,
        message_id: str = None
    ) -> AsyncGenerator[str, None]:
        """æµå¼å¤„ç†ç”¨æˆ·æ¶ˆæ¯
        
        Args:
            platform: å¹³å°åç§°
            user_id: ç”¨æˆ·ID
            text: æ¶ˆæ¯å†…å®¹
            chat_id: èŠå¤©ID
            message_id: æ¶ˆæ¯ID
            
        Yields:
            æµå¼å“åº”ç‰‡æ®µ
        """
        # å…ˆå‘é€ç¡®è®¤
        yield "â³ æ­£åœ¨å¤„ç†..."
        
        # å¤„ç†æ¶ˆæ¯
        response = await self.handle_message(
            platform=platform,
            user_id=user_id,
            text=text,
            chat_id=chat_id,
            message_id=message_id
        )
        
        # æ¨¡æ‹Ÿæµå¼è¾“å‡ºï¼ˆæŒ‰æ®µè½åˆ†å‰²ï¼‰
        paragraphs = response.split('\n\n')
        for i, para in enumerate(paragraphs):
            if i > 0:
                yield '\n\n'
            yield para
    
    def _format_memories(self, memories: list) -> str:
        """æ ¼å¼åŒ–è®°å¿†ä¸ºä¸Šä¸‹æ–‡"""
        if not memories:
            return ""
        return "\n".join(f"- {m['content'][:200]}" for m in memories)
    
    async def get_stats(self) -> dict:
        """è·å– Agent ç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            'version': self.config.version,
            'running': self._running,
            'identity': self.identity.get_identity_summary() if self.identity else None,
            'skills': {
                'native': len(self.skills.skills) if self.skills else 0,
                'openclaw': len(self.openclaw_skills.skills) if self.openclaw_skills else 0
            },
            'memory': self.memory.get_stats() if self.memory else None,
            'tasks': self.task_queue.get_stats() if self.task_queue else None,
            'worker': self.task_worker.get_stats() if self.task_worker else None,
            'sessions': self.chat_manager.get_stats() if self.chat_manager else None
        }
        return stats
